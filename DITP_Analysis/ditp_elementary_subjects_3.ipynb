{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Number of feedbacks retrieved: 100\n",
      "INFO:__main__:Number of rows pending elementary_subjects processing: 100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import certifi\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import boto3\n",
    "\n",
    "from utils.analysis_utils import (\n",
    "    format_ligne,\n",
    "    get_elementary_subjects_for_part_of_feedback,\n",
    "    apply_topic_processing\n",
    ")\n",
    "from utils.database import update_feedbacks, insert_new_elementary_subjects\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "# Constants\n",
    "LANGUAGE = 'french'\n",
    "BRAND_NAME = 'ditp_analysis'\n",
    "MONGO_SECRET_ID = 'Prod/alloreview'\n",
    "MONGO_REGION = 'eu-west-3'\n",
    "MONGO_DATABASE = 'feedbacks_db'\n",
    "MONGO_COLLECTION = 'feedbacks_Prod'\n",
    "SAMPLE_SIZE = 100  # Adjust as needed\n",
    "MODEL_NAME = 'gpt-4o-mini'  # Adjust as needed\n",
    "\n",
    "def get_mongo_client():\n",
    "    \"\"\"\n",
    "    Establishes a connection to the MongoDB client using credentials from AWS Secrets Manager or environment variables.\n",
    "    \"\"\"\n",
    "    mongo_uri = os.getenv('MONGO_CONNECTION_STRING')\n",
    "    if not mongo_uri:\n",
    "        secrets_manager_client = boto3.client(\"secretsmanager\", region_name=MONGO_REGION)\n",
    "        secrets = json.loads(\n",
    "            secrets_manager_client.get_secret_value(\n",
    "                SecretId=MONGO_SECRET_ID\n",
    "            )[\"SecretString\"]\n",
    "        )\n",
    "        password = secrets[\"mongodb\"][\"password\"]\n",
    "        mongo_uri = f\"mongodb+srv://alloreview:{password}@feedbacksdev.cuwx1.mongodb.net\"\n",
    "\n",
    "    return MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "\n",
    "mongo_client = get_mongo_client()\n",
    "collection = mongo_client[MONGO_DATABASE][MONGO_COLLECTION]\n",
    "\n",
    "def get_feedbacks_to_process(collection, brand_name, sample_size):\n",
    "    \"\"\"\n",
    "    Retrieves feedback documents from MongoDB that need processing.\n",
    "\n",
    "    :param collection: MongoDB collection object.\n",
    "    :param brand_name: Name of the brand to filter.\n",
    "    :param sample_size: Number of documents to sample.\n",
    "    :return: DataFrame containing feedbacks to process.\n",
    "    \"\"\"\n",
    "    query = {\n",
    "        '$and': [\n",
    "            {'brand': brand_name},\n",
    "            {'extractions': {'$exists': True, '$not': {'$size': 0}}},\n",
    "            {'splitted_analysis_v2': {'$exists': True, '$not': {'$size': 0}}},\n",
    "            {'extractions': {'$not': {'$elemMatch': {'elementary_subjects': {'$exists': True}}}}}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    pipeline = [\n",
    "        {'$match': query},\n",
    "        {'$sample': {'size': sample_size}}\n",
    "    ]\n",
    "\n",
    "    feedbacks_cursor = collection.aggregate(pipeline)\n",
    "    feedbacks = list(feedbacks_cursor)\n",
    "    return pd.DataFrame(feedbacks)\n",
    "\n",
    "df_feedbacks = get_feedbacks_to_process(collection, BRAND_NAME, SAMPLE_SIZE)\n",
    "logger.info(f\"Number of feedbacks retrieved: {df_feedbacks.shape[0]}\")\n",
    "\n",
    "# Select relevant columns\n",
    "df_feedbacks = df_feedbacks[[\n",
    "    '_id', 'ecrit_le', 'splitted_analysis_v2', 'extractions',\n",
    "    'intitule_structure_1', 'intitule_structure_2', 'tags_metiers', 'pays', 'verbatims'\n",
    "]]\n",
    "\n",
    "# Generate brand_context column\n",
    "df_feedbacks['brand_context'] = df_feedbacks.apply(format_ligne, axis=1)\n",
    "\n",
    "# Prepare DataFrame for processing\n",
    "df_to_process = df_feedbacks[['_id', 'extractions', 'brand_context']]\n",
    "logger.info(f\"Number of rows pending elementary_subjects processing: {df_to_process.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Explode 'extractions' to have one extraction per row\n",
    "df_extractions = df_to_process.explode('extractions').reset_index(drop=True)\n",
    "\n",
    "def process_extraction_row(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Processes a single extraction row to add elementary_subjects.\n",
    "\n",
    "    :param row: Pandas Series representing a row with '_id', 'extractions', and 'brand_context'.\n",
    "    :return: Updated row with 'extractions' and 'elementary_subjects' fields.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        extraction = row['extractions']\n",
    "        extraction, elementary_subjects = get_elementary_subjects_for_part_of_feedback(\n",
    "            extractions=extraction,\n",
    "            language=LANGUAGE,\n",
    "            brand_name=BRAND_NAME,\n",
    "            brand_context=row['brand_context'],\n",
    "            model=MODEL_NAME,\n",
    "            should_update_mongo=False\n",
    "        )\n",
    "        row['extractions'] = extraction\n",
    "        row['elementary_subjects'] = elementary_subjects\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing extraction for _id {row['_id']}: {e}\")\n",
    "    return row\n",
    "\n",
    "def process_extractions_in_parallel(df: pd.DataFrame, func, max_workers=10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes the extractions DataFrame in parallel using threads.\n",
    "\n",
    "    :param df: DataFrame containing extractions to process.\n",
    "    :param func: Function to apply to each row.\n",
    "    :param max_workers: Maximum number of worker threads.\n",
    "    :return: DataFrame with processed extractions.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(func, row): idx for idx, row in df.iterrows()}\n",
    "        for future in as_completed(futures):\n",
    "            idx = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing row at index {idx}: {e}\")\n",
    "    processed_df = pd.DataFrame(results)\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process extractions in parallel\n",
    "df_processed_extractions = process_extractions_in_parallel(df_extractions, process_extraction_row)\n",
    "\n",
    "# Group by '_id' and aggregate 'extractions' into lists\n",
    "df_grouped = df_processed_extractions.groupby('_id').agg({\n",
    "    'extractions': list\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.database import (get_elementary_subjects)\n",
    "\n",
    "# First, retrieve existing elementary subjects per sentiment type\n",
    "existing_topics_by_sentiment = {}\n",
    "for sentiment in ['negative', 'positive', 'suggestion']:\n",
    "    existing_subjects = get_elementary_subjects(BRAND_NAME, sentiment)\n",
    "    existing_topics_by_sentiment[sentiment.upper()] = [item['elementary_subject'] for item in existing_subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "def process_row(extractions_row):\n",
    "    \"\"\"\n",
    "    Wrapper function to process a single row of extractions.\n",
    "    \"\"\"\n",
    "    return apply_topic_processing(extractions_row, existing_topics_by_sentiment)\n",
    "\n",
    "# Number of worker threads\n",
    "MAX_WORKERS = 10  # Adjust this number based on your system and API rate limits\n",
    "\n",
    "# Apply the function in parallel using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Create a mapping of futures\n",
    "    future_to_index = {\n",
    "        executor.submit(process_row, row): idx for idx, row in df_grouped['extractions'].items()  # Use items() instead of iteritems()\n",
    "    }\n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(future_to_index):\n",
    "        idx = future_to_index[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            df_grouped.at[idx, 'extractions'] = result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing row {idx}: {e}\")\n",
    "            # Handle the error as needed (e.g., assign None or keep the original value)\n",
    "            df_grouped.at[idx, 'extractions'] = df_grouped.at[idx, 'extractions']  # Or assign a default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_elementary_subjects = {}\n",
    "# for _, row in df_grouped.iterrows():\n",
    "#     extractions = row['extractions']\n",
    "#     for extraction in extractions:\n",
    "#         subjects = extraction.get('elementary_subjects', [])\n",
    "#         sentiment = extraction.get('sentiment', 'UNKNOWN')  # Default to UNKNOWN if sentiment is missing\n",
    "#         for subject in subjects:\n",
    "#             # Store the elementary_subject along with its sentiment as type\n",
    "#             all_elementary_subjects[subject] = sentiment\n",
    "\n",
    "# # Prepare the list of elementary_subjects to insert\n",
    "# subjects_to_insert = [\n",
    "#     {\n",
    "#         \"elementary_subject\": subject,\n",
    "#         \"type\": sentiment,  # Use the sentiment as type\n",
    "#     }\n",
    "#     for subject, sentiment in all_elementary_subjects.items()\n",
    "# ]\n",
    "# subjects_to_insert\n",
    "# subject_names = [subject['elementary_subject'] for subject in subjects_to_insert]\n",
    "# subject_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def bulk_insert_elementary_subjects(df, insert_function, brand):\n",
    "    \"\"\"\n",
    "    Prepares and performs bulk insertion of new elementary_subjects into MongoDB\n",
    "    for each row in the DataFrame.\n",
    "\n",
    "    :param df: DataFrame containing 'extractions' column.\n",
    "    :param insert_function: Function to perform the insertion in MongoDB.\n",
    "    :param brand: Brand name to associate with the elementary_subjects.\n",
    "    \"\"\"\n",
    "    # Collect all elementary_subjects from the DataFrame along with their types (sentiment)\n",
    "    all_elementary_subjects = {}\n",
    "    for _, row in df.iterrows():\n",
    "        extractions = row['extractions']\n",
    "        for extraction in extractions:\n",
    "            subjects = extraction.get('elementary_subjects', [])\n",
    "            sentiment = extraction.get('sentiment', 'UNKNOWN')  # Default to UNKNOWN if sentiment is missing\n",
    "            for subject in subjects:\n",
    "                # Store the elementary_subject along with its sentiment as type\n",
    "                all_elementary_subjects[subject] = sentiment\n",
    "\n",
    "    # Prepare the list of elementary_subjects to insert\n",
    "    subjects_to_insert = [\n",
    "        {\n",
    "            \"elementary_subject\": subject,\n",
    "            \"type\": sentiment,  # Use the sentiment as type\n",
    "        }\n",
    "        for subject, sentiment in all_elementary_subjects.items()\n",
    "    ]\n",
    "\n",
    "    # Call the insert function with the list of subjects to insert\n",
    "    insert_function(subjects_to_insert, brand)\n",
    "\n",
    "bulk_insert_elementary_subjects(\n",
    "    df_grouped,\n",
    "    insert_new_elementary_subjects,\n",
    "    brand=BRAND_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.analysis_utils import update_splitted_analysis\n",
    "df_grouped['splitted_analysis_v2'] = df_grouped.apply(\n",
    "    lambda row: update_splitted_analysis(row['_id'], row['extractions'], 'splitted_analysis_v2'), \n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 100 feedbacks\n",
      "Matched: 100, Modified: 100\n"
     ]
    }
   ],
   "source": [
    "# Update feedbacks in MongoDB\n",
    "def bulk_update_feedbacks_from_dataframe(df, update_function):\n",
    "    \"\"\"\n",
    "    Prepares and performs bulk update of feedback documents in MongoDB.\n",
    "\n",
    "    :param df: DataFrame containing '_id' and 'extractions' columns.\n",
    "    :param update_function: Function to perform the update in MongoDB.\n",
    "    \"\"\"\n",
    "    feedbacks_to_update = [\n",
    "        {\n",
    "            \"id\": row['_id'],\n",
    "            \"updates\": {\n",
    "                \"extractions\": row['extractions'],\n",
    "                \"splitted_analysis_v2\": row['splitted_analysis_v2'],\n",
    "                \"topics_v2\": []\n",
    "                # Include other fields to update if necessary\n",
    "            }\n",
    "        }\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "    # Call the update function with the list of feedback updates\n",
    "    update_function(feedbacks_to_update)\n",
    "\n",
    "bulk_update_feedbacks_from_dataframe(df_grouped, update_feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
